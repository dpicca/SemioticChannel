# Semiotic Channel Principle — Taboo Experiment

Implementation of the "Inverse Definition Game" (Taboo Semiotic) to validate the **Semiotic Channel Principle**.

## Core Hypothesis

The semiotic channel operates locally. At increasing temperature ($\lambda$):
- **S (Semiotic Breadth)** increases — the message space expands.
- **D (Decipherability)** follows an inverted-U curve — rising with initial diversity but collapsing as the message becomes incoherent.
- **C (Semiotic Capacity)** = $\max(D)$ — the optimal balance point for communication.

## Information-Theoretic Metrics

This implementation strictly adheres to the formal definitions provided in the paper *Semiotic Channel Principle*:

1.  **Semiotic Breadth ($S$)**: Measures the diversity of the message space. 
    - **Formula**: $S = 2^{H(M)}$ (Perplexity)
    - Calculated as the perplexity of the distribution of message clusters (clustered embeddings of generated descriptions).

2.  **Decipherability ($D$)**: Measures the information transferred effectively to the receiver.
    - **Formula**: $D = I(M; Int)$ (Mutual Information)
    - Calculated as the Mutual Information between the message clusters ($M$) and the discrete interpretations ($Int$) generated by the receiver.

## Experiment Architecture

```
┌─────────────────────────────────────────────────────────────┐
│              ROBUST TABOO SEMIOTIC EXPERIMENT               │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  ┌─────────────┐    ┌──────────────┐    ┌───────────────┐  │
│  │   EMITTER   │───▶│  DESCRIPTION │───▶│   RECEIVER    │  │
│  │  (LLM @ λ)  │    │   (Message)  │    │ (LLM @ 0.2)   │  │
│  └─────────────┘    └──────────────┘    └───────────────┘  │
│         │                  │                    │          │
│         ▼                  ▼                    ▼          │
│  ┌─────────────┐    ┌──────────────┐    ┌───────────────┐  │
│  │   Concept   │    │  Clustering  │    │ Interpretation│  │
│  │  (Target)   │    │ (Embeddings) │    │    (Guess)    │  │
│  └─────────────┘    └──────────────┘    └───────────────┘  │
│                            │                    │          │
│                            ▼                    ▼          │
│                     CALCULATE PPL(M)     CALCULATE MI       │
│                           (S)           I(M; Int) (D)       │
│                                                             │
│  LOGGING: experiments/logs/*.jsonl (Turn-level data)        │
│  RESULTS: S (Perplexity) vs D (Mutual Information)          │
└─────────────────────────────────────────────────────────────┘
```

## Project Structure

```
semiotic_channel/
├── concepts_dataset.py   # Dataset handling
├── llm_interface.py      # Agno/Ollama Interface & Embeddings
└── metrics.py            # Perplexity (S) and Mutual Information (D)

experiments/
├── taboo_experiment.py   # Main CLI pipeline (Cross-Model Capable)
├── results/              # Aggregated JSON results & Plots
└── logs/                 # JSONL files (Turn-level logs)

web/
└── server.py             # Flask Web Interface
```

## Quick Start
Run a rigorous experiment with turn-level logging and formal metrics.

```bash
# Setup
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Run full experiment (Self-Play)
# Requires Ollama running
export PYTHONPATH=$PYTHONPATH:.
python3 experiments/taboo_experiment.py \
    --n-concepts 50 \
    --n-descriptions 10 \
    --model llama3:latest
```

This will produce:
- **Log**: `experiments/logs/experiment_turns_YYYYMMDD_HHMMSS.jsonl`
- **Result**: `experiments/results/experiment_YYYYMMDD_HHMMSS.json`
- **Plot**: `semiotic_profile_comparison.png`

## Requirements

- **Ollama** running locally (`ollama serve`)
- Models: `llama3:latest` (or others), `nomic-embed-text:latest` (for embeddings/clustering)
- Python 3.10+
- Dependencies: `agno`, `scikit-learn`, `flask`, `numpy`, `matplotlib`
