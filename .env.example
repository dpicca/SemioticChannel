# Semiotic Channel - Ollama Configuration
# Copy this file to .env and fill in your settings

# Ollama host (default: local)
OLLAMA_HOST=http://localhost:11434

# Default model for experiments
# Recommended for Mac M1: llama3.2:1b, phi3:mini, gemma2:2b
DEFAULT_MODEL=llama3.2:1b

# Embedding model (must be pulled: ollama pull nomic-embed-text)
EMBEDDING_MODEL=nomic-embed-text
